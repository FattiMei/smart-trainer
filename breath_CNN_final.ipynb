{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ea456197"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "file_list = os.listdir('/content/SR250Breath')\n",
        "windows = []\n",
        "labels = []\n",
        "\n",
        "for file_name in file_list:\n",
        "    file_path = os.path.join('/content/SR250Breath', file_name)\n",
        "    try:\n",
        "        data = np.load(file_path)\n",
        "\n",
        "        if file_path.endswith('.window.npy'):\n",
        "            windows.append(data)\n",
        "        elif file_path.endswith('.label.npy'):\n",
        "            labels.append(data)\n",
        "        else:\n",
        "            print(f\"Unknown file type: {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not load {file_name}: {e}\")\n",
        "\n",
        "# You can access the arrays using the file names as keys, for example:\n",
        "# print(data_arrays['Millenials_E_breath_sitting_desk_20250909-115853.window.npy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "windows = np.concat(windows)\n",
        "labels = np.concat(labels)\n",
        "\n",
        "print(windows.shape)\n",
        "print(labels.shape)"
      ],
      "metadata": {
        "id": "CKLT-yDEqQSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# in maniera tale che l'ultimo asse corrisponde alle antenne\n",
        "windows = windows.transpose(0, 1, 3, 2)\n",
        "print(windows.shape)"
      ],
      "metadata": {
        "id": "516DnmAorrio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "abs_windows = np.abs(windows)\n",
        "print(abs_windows.shape)"
      ],
      "metadata": {
        "id": "tYQi2nSqrjCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phase_windows = np.angle(windows)"
      ],
      "metadata": {
        "id": "NNyyGRhSzWuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all = np.concatenate((abs_windows, phase_windows), axis=-1)\n",
        "# all = phase_windows\n",
        "print(all.shape)"
      ],
      "metadata": {
        "id": "Nw1pRFp-1LDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Assuming 'labels' is your numpy array of labels\n",
        "# You might need to adjust the 'depth' parameter based on the number of unique classes in your labels\n",
        "one_hot_labels = tf.one_hot(labels, depth=2) # Assuming 2 classes for demonstration\n",
        "\n",
        "print(one_hot_labels.shape)"
      ],
      "metadata": {
        "id": "Qr6jixidsvWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "887de8ae"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Convert TensorFlow tensor to NumPy array\n",
        "one_hot_labels_np = one_hot_labels.numpy()\n",
        "\n",
        "# Split into training and temporary sets (for test and validation)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(phase_windows, one_hot_labels_np, test_size=0.3, random_state=42)\n",
        "\n",
        "# Split the temporary set into test and validation sets\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "print(\"Training set shapes:\")\n",
        "print(\"X_train:\", X_train.shape)\n",
        "print(\"y_train:\", y_train.shape)\n",
        "print(\"\\nValidation set shapes:\")\n",
        "print(\"X_val:\", X_val.shape)\n",
        "print(\"y_val:\", y_val.shape)\n",
        "print(\"\\nTest set shapes:\")\n",
        "print(\"X_test:\", X_test.shape)\n",
        "print(\"y_test:\", y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e66663bd"
      },
      "source": [
        "# Task\n",
        "Produce a CNN model for the prediction of the label and train it using the training, test, and validation data split from the `abs_windows` and `labels` variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57437bfa"
      },
      "source": [
        "## Define the cnn model architecture\n",
        "\n",
        "### Subtask:\n",
        "Define the layers of the CNN model, including convolutional layers, pooling layers, and dense layers, suitable for your data shape.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e63854b"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the CNN model architecture using convolutional, pooling, and dense layers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "796dbb6f"
      },
      "source": [
        "**Reasoning**:\n",
        "The error indicates that the input shape is too small for the convolutional and pooling layers. Adjust the kernel size and pooling size to be smaller to avoid negative dimensions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7bHbNiLtgjZ"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Add convolutional layers with adjusted kernel and pooling sizes\n",
        "model.add(Conv2D(16, (1, 1), activation='relu', input_shape=X_train.shape[1:]))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(16, (2, 2), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(16, (1, 1), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 4)))\n",
        "\n",
        "# Flatten the output\n",
        "model.add(Flatten())\n",
        "\n",
        "# Add dense layers\n",
        "# model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(y_train.shape[1], activation='sigmoid'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model_little = model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Add convolutional layers with adjusted kernel and pooling sizes\n",
        "model.add(Conv2D(32, (1, 1), activation='relu', input_shape=X_train.shape[1:]))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (2, 2), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(128, (1, 1), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 4)))\n",
        "\n",
        "# Flatten the output\n",
        "model.add(Flatten())\n",
        "\n",
        "# Add dense layers\n",
        "# model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(y_train.shape[1], activation='sigmoid'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model_small = model"
      ],
      "metadata": {
        "id": "9nITO8FT59FR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Add convolutional layers with adjusted kernel and pooling sizes\n",
        "model.add(Conv2D(32, (2, 2), activation='relu', input_shape=X_train.shape[1:]))\n",
        "model.add(MaxPooling2D((1, 2))) # Pool only along the second dimension\n",
        "model.add(Conv2D(64, (2, 2), activation='relu'))\n",
        "model.add(MaxPooling2D((1, 2)))\n",
        "model.add(Conv2D(128, (2, 2), activation='relu'))\n",
        "model.add(MaxPooling2D((1, 2)))\n",
        "\n",
        "\n",
        "# Flatten the output\n",
        "model.add(Flatten())\n",
        "\n",
        "# Add dense layers\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(y_train.shape[1], activation='sigmoid'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model_medium = model"
      ],
      "metadata": {
        "id": "AlBLgaNrQ9Et"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Add convolutional layers with further adjusted kernel and pooling sizes\n",
        "model.add(Conv2D(32, (1, 1), activation='relu', input_shape=X_train.shape[1:]))\n",
        "model.add(MaxPooling2D((1, 1))) # Use (1,1) pooling to avoid reducing dimension too quickly\n",
        "model.add(Conv2D(64, (1, 1), activation='relu'))\n",
        "model.add(MaxPooling2D((1, 1)))\n",
        "model.add(Conv2D(128, (1, 1), activation='relu'))\n",
        "model.add(MaxPooling2D((1, 1)))\n",
        "\n",
        "\n",
        "# Flatten the output\n",
        "model.add(Flatten())\n",
        "\n",
        "# Add dense layers\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(y_train.shape[1], activation='sigmoid'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model_large = model"
      ],
      "metadata": {
        "id": "CLAErSOiQREl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0d0d6cb"
      },
      "source": [
        "**Reasoning**:\n",
        "The model summary shows that the output shape after the last MaxPooling2D layer has a dimension of 0, which is incorrect. The input shape is likely still too large for the kernel and pooling sizes. I need to further adjust the kernel and pooling sizes to ensure the output dimensions remain positive.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TFFVXwqtiwn"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Add convolutional layers with adjusted kernel and pooling sizes\n",
        "model.add(Conv2D(32, (1, 1), activation='relu', input_shape=X_train.shape[1:]))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (2, 2), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(128, (1, 1), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 4)))\n",
        "\n",
        "\n",
        "# Flatten the output\n",
        "model.add(Flatten())\n",
        "\n",
        "# Add dense layers\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(y_train.shape[1], activation='sigmoid'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model_small_test1 = model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56fb84ff"
      },
      "source": [
        "**Reasoning**:\n",
        "The model summary now shows positive dimensions after the pooling layers, but using (1,1) kernel and pooling sizes essentially bypasses the spatial feature extraction capabilities of CNNs. I need to find a balance between kernel/pooling sizes and the input shape to effectively use convolutional layers without causing negative dimensions. I will try increasing the kernel size slightly while keeping the pooling size small.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cJGuhfotlXj"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Add convolutional layers with adjusted kernel and pooling sizes\n",
        "model.add(Conv2D(32, (2, 2), activation='relu', input_shape=X_train.shape[1:]))\n",
        "model.add(MaxPooling2D((1, 2))) # Pool only along the second dimension\n",
        "model.add(Conv2D(64, (2, 2), activation='relu'))\n",
        "model.add(MaxPooling2D((1, 2)))\n",
        "model.add(Conv2D(128, (2, 2), activation='relu'))\n",
        "model.add(MaxPooling2D((1, 2)))\n",
        "\n",
        "\n",
        "# Flatten the output\n",
        "model.add(Flatten())\n",
        "\n",
        "# Add dense layers\n",
        "# model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(y_train.shape[1], activation='sigmoid'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model_small_test2 = model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "391c3f47"
      },
      "source": [
        "## Compile the model\n",
        "\n",
        "### Subtask:\n",
        "Compile the CNN model by specifying the optimizer, loss function, and metrics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41732786"
      },
      "source": [
        "**Reasoning**:\n",
        "Compile the CNN model with the Adam optimizer, binary crossentropy loss, and accuracy metric.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86ebf13e"
      },
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "model = model_small_test2\n",
        "\n",
        "model.compile(optimizer=Adam(),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14a41350"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "### Subtask:\n",
        "Train the compiled model using the training data (`X_train`, `y_train`) and validate it using the validation data (`X_val`, `y_val`).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cedb53a9"
      },
      "source": [
        "**Reasoning**:\n",
        "Train the compiled model using the training and validation data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23c88a49"
      },
      "source": [
        "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50993fe7"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step is to load the data from the CSV file into a pandas DataFrame and display the first few rows to understand its structure.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eb47ca2"
      },
      "source": [
        "## Evaluate the model\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the trained model on the test data (`X_test`, `y_test`) to assess its performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d47c36c"
      },
      "source": [
        "**Reasoning**:\n",
        "Evaluate the trained model on the test data to assess its performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1e1d9ace"
      },
      "source": [
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "\n",
        "print(f\"Test Loss: {loss}\")\n",
        "print(f\"Test Accuracy: {accuracy}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "# Get predictions from the model\n",
        "y_pred_prob = model.predict(X_test)\n",
        "\n",
        "# Convert probabilities to class predictions (assuming binary classification and sigmoid activation)\n",
        "# Since y_test is one-hot encoded, y_pred should also be one-hot encoded for comparison with some metrics\n",
        "# However, precision_score, recall_score, and f1_score with average='binary' expect binary labels.\n",
        "# Let's convert both to binary labels for these metrics.\n",
        "y_pred = (y_pred_prob[:, 1] > 0.5).astype(int) # Assuming the second column is the positive class probability\n",
        "\n",
        "# Convert one-hot encoded true labels back to single class labels\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Calculate metrics\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1 Score: {f1}\")"
      ],
      "metadata": {
        "id": "4H8eDeT6jTET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Quantize the model\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "quantized_tflite_model = converter.convert()\n",
        "\n",
        "# Save the quantized model (optional)\n",
        "# with open('quantized_model.tflite', 'wb') as f:\n",
        "#     f.write(quantized_tflite_model)\n",
        "\n",
        "# Load the quantized model\n",
        "interpreter = tf.lite.Interpreter(model_content=quantized_tflite_model)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get input and output tensors\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "# Prepare test data for inference\n",
        "# TFLite models expect float32 inputs\n",
        "X_test_float32 = X_test.astype(np.float32)\n",
        "\n",
        "# Run inference on the test set\n",
        "predictions = []\n",
        "for i in range(X_test_float32.shape[0]):\n",
        "    interpreter.set_tensor(input_details[0]['index'], np.expand_dims(X_test_float32[i], axis=0))\n",
        "    interpreter.invoke()\n",
        "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "    predictions.append(output_data[0])\n",
        "\n",
        "predictions = np.array(predictions)\n",
        "\n",
        "# Evaluate the quantized model (assuming y_test is already one-hot encoded)\n",
        "# For binary classification with sigmoid output, you can use binary_accuracy\n",
        "from tensorflow.keras.metrics import BinaryAccuracy\n",
        "\n",
        "binary_accuracy = BinaryAccuracy()\n",
        "binary_accuracy.update_state(y_test, predictions)\n",
        "\n",
        "print(f\"Quantized Model Test Accuracy: {binary_accuracy.result().numpy()}\")\n",
        "\n",
        "# If you want to calculate loss, you'd need to implement it manually or convert predictions to match y_test format\n",
        "# For example, if y_test is one-hot, and predictions are probabilities for each class:\n",
        "# from tensorflow.keras.losses import BinaryCrossentropy\n",
        "# bce = BinaryCrossentropy()\n",
        "# loss = bce(y_test, predictions).numpy()\n",
        "# print(f\"Quantized Model Test Loss: {loss}\")"
      ],
      "metadata": {
        "id": "SUFu4NnP0VZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "# predictions from the quantized model were already calculated in cell SUFu4NnP0VZO\n",
        "\n",
        "# Convert probabilities to class predictions (assuming binary classification and sigmoid activation)\n",
        "y_pred_quantized = (predictions[:, 1] > 0.5).astype(int) # Assuming the second column is the positive class probability\n",
        "\n",
        "# Convert one-hot encoded true labels back to single class labels\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Calculate metrics for the quantized model\n",
        "precision_quantized = precision_score(y_true, y_pred_quantized)\n",
        "recall_quantized = recall_score(y_true, y_pred_quantized)\n",
        "f1_quantized = f1_score(y_true, y_pred_quantized)\n",
        "\n",
        "print(f\"Quantized Model Precision: {precision_quantized}\")\n",
        "print(f\"Quantized Model Recall: {recall_quantized}\")\n",
        "print(f\"Quantized Model F1 Score: {f1_quantized}\")"
      ],
      "metadata": {
        "id": "vToMEXfNkXRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Save the quantized model temporarily to check its size\n",
        "quantized_model_path = 'quantized_model.tflite'\n",
        "with open(quantized_model_path, 'wb') as f:\n",
        "    f.write(quantized_tflite_model)\n",
        "\n",
        "# Get the file size in bytes and convert to kilobytes\n",
        "model_size_bytes = os.path.getsize(quantized_model_path)\n",
        "model_size_kb = model_size_bytes / 1024\n",
        "\n",
        "print(f\"Dimensioni del modello quantizzato: {model_size_bytes} bytes ({model_size_kb:.2f} KB)\")\n",
        "\n",
        "# Clean up the temporary file\n",
        "os.remove(quantized_model_path)"
      ],
      "metadata": {
        "id": "BAESa-8t2YPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.mkdir('/content/models')\n",
        "\n",
        "model.save(os.path.join('/content/models', 'model_small_test2' + '2.keras'))"
      ],
      "metadata": {
        "id": "yjnN_EXZZGd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Float model export:\n",
        "# The second argument, string format, must be the model name + 2.keras\n",
        "model_small_test2 = tf.keras.models.load_model(os.path.join('/content/models', 'model_small_test22.keras'))\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model_small_test2)\n",
        "tflite_model = converter.convert()\n",
        "print(\"Float model size:\", open(os.path.join('/content/models', 'model_small_test2' + '.tflite'), \"wb\").write(tflite_model))"
      ],
      "metadata": {
        "id": "oInWVr7CWQYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train[0].shape)"
      ],
      "metadata": {
        "id": "s5YE1O1MZ0Ny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantized model export:\n",
        "\n",
        "# Definition of Representative Dataset generator:\n",
        "def representative_data_gen():\n",
        "  for i in range(X_train.shape[0]):\n",
        "    yield [X_train[i].reshape((-1,) + X_train[i].shape)]\n",
        "\n",
        "#def representative_dataset():\n",
        "#  for i in range(100):\n",
        "#    yield [ np.array([(np.random.rand(1960)).astype(np.float32)]) ]\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model_small_test2)\n",
        "\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.representative_dataset = representative_data_gen\n",
        "\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "converter.inference_input_type = tf.compat.v1.lite.constants.INT8 # or tf.uint8\n",
        "converter.inference_output_type = tf.compat.v1.lite.constants.INT8  # or tf.uint8\n",
        "\n",
        "tflite_model_quant = converter.convert()\n",
        "print(\"Quantized model size: \", open(os.path.join('/content/models', 'model_small_test2' + '-int8.tflite'), \"wb\").write(tflite_model_quant))"
      ],
      "metadata": {
        "id": "ZLQ9dus_YkUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update && apt-get -qq install xxd\n",
        "\n",
        "MODEL_TFLITE = '/content/models/'+ 'model_small_test2' +'-int8.tflite'\n",
        "MODEL_TFLITE_MICRO = 'TinyConvModel-int8.cc'"
      ],
      "metadata": {
        "id": "R0evwEG_Ynlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!xxd -i {MODEL_TFLITE} > {MODEL_TFLITE_MICRO}"
      ],
      "metadata": {
        "id": "QZeCg9bUcGij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cC3WmLRrxYIY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}